#!/usr/bin/bash

finally() {
    trap - ERR EXIT
    if [ -f Qwen3-1.7B/model-00002-of-00002.safetensors.bak ]; then
        if [ -f Qwen3-1.7B/model-00002-of-00002.safetensors ]; then
            rm Qwen3-1.7B/model-00002-of-00002.safetensors
        fi
        mv Qwen3-1.7B/model-00002-of-00002.safetensors.bak Qwen3-1.7B/model-00002-of-00002.safetensors
    fi
}

trap finally ERR EXIT INT TERM

set -e
if ! [ -d Qwen3-1.7B ]; then
    git clone https://huggingface.co/Qwen/Qwen3-1.7B --no-depth
fi
if ! [ -d llama.cpp ]; then
    git clone https://github.com/ggml-org/llama.cpp --no-depth
fi

cp Qwen3-1.7B/model-00002-of-00002.safetensors Qwen3-1.7B/model-00002-of-00002.safetensors.bak
python tools/embedding_modifier.py Qwen3-1.7B/model-00002-of-00002.safetensors lm_head[151668]=lm_head[151667] \
lm_head[151667]=zero --inplace
python llama.cpp/convert_hf_to_gguf.py Qwen3-1.7B --outfile qwen-bf16.gguf --outtype bf16

quant_precisions=("iq3_xs" "iq4_nl" "iq4_xs" "q3_k_l" "q4_k_m" "q5_k_m" "q6_k" "q8_0")
mkdir -p qwen3-1.7b-gguf
for prec in "${quant_precisions[@]}"; do
    # requires llama-quantize command in PATH
    llama-quantize qwen-bf16.gguf qwen3-1.7b-gguf/qwen3-1.7b-$prec.gguf $prec
done
rm qwen-bf16.gguf
